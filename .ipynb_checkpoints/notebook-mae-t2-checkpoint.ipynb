{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc796e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 06:26:28.090887: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-30 06:26:28.205088: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-30 06:26:28.229926: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-30 06:26:28.672670: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-30 06:26:28.672716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-30 06:26:28.672721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score, matthews_corrcoef\n",
    "from utils.dataset import MARIO_DS, MARIO_DS_T2\n",
    "from utils.scoring import specificity, compute_metrics, plot_confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from models.df41_model_task1 import MarioModelT1\n",
    "# from models.example_model_task1v2 import SimpleModel1v2\n",
    "\n",
    "from models.mae_model import *\n",
    "from models.mae_utils import *\n",
    "\n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision.transforms as T\n",
    "\n",
    "import operator\n",
    "import functools\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01a61b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_patient</th>\n",
       "      <th>side_eye</th>\n",
       "      <th>BScan</th>\n",
       "      <th>image_at_ti</th>\n",
       "      <th>image_at_ti+1</th>\n",
       "      <th>split_type</th>\n",
       "      <th>LOCALIZER_at_ti+1</th>\n",
       "      <th>LOCALIZER_at_ti</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_at_ti+1</th>\n",
       "      <th>age_at_ti</th>\n",
       "      <th>num_current_visit_at_i+1</th>\n",
       "      <th>num_current_visit_at_i</th>\n",
       "      <th>delta_t</th>\n",
       "      <th>label</th>\n",
       "      <th>case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>OS</td>\n",
       "      <td>2</td>\n",
       "      <td>B3944840.png</td>\n",
       "      <td>AA8E1A00.png</td>\n",
       "      <td>val</td>\n",
       "      <td>A9D689D0.png</td>\n",
       "      <td>B2EB0FF0.png</td>\n",
       "      <td>M</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>OS</td>\n",
       "      <td>3</td>\n",
       "      <td>B38D4360.png</td>\n",
       "      <td>AA86EE10.png</td>\n",
       "      <td>val</td>\n",
       "      <td>A9D689D0.png</td>\n",
       "      <td>B2EB0FF0.png</td>\n",
       "      <td>M</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>OS</td>\n",
       "      <td>4</td>\n",
       "      <td>B3813570.png</td>\n",
       "      <td>AA7D7830.png</td>\n",
       "      <td>val</td>\n",
       "      <td>A9D689D0.png</td>\n",
       "      <td>B2EB0FF0.png</td>\n",
       "      <td>M</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>OS</td>\n",
       "      <td>5</td>\n",
       "      <td>B37A3090.png</td>\n",
       "      <td>AA73DB40.png</td>\n",
       "      <td>val</td>\n",
       "      <td>A9D689D0.png</td>\n",
       "      <td>B2EB0FF0.png</td>\n",
       "      <td>M</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>OS</td>\n",
       "      <td>6</td>\n",
       "      <td>B37304A0.png</td>\n",
       "      <td>AA6A6560.png</td>\n",
       "      <td>val</td>\n",
       "      <td>A9D689D0.png</td>\n",
       "      <td>B2EB0FF0.png</td>\n",
       "      <td>M</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>134</td>\n",
       "      <td>OS</td>\n",
       "      <td>16</td>\n",
       "      <td>581D2870.png</td>\n",
       "      <td>849B3A90.png</td>\n",
       "      <td>val</td>\n",
       "      <td>84586530.png</td>\n",
       "      <td>57EFD6E0.png</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>29369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>134</td>\n",
       "      <td>OS</td>\n",
       "      <td>17</td>\n",
       "      <td>58138B80.png</td>\n",
       "      <td>84919DA0.png</td>\n",
       "      <td>val</td>\n",
       "      <td>84586530.png</td>\n",
       "      <td>57EFD6E0.png</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>29370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>134</td>\n",
       "      <td>OS</td>\n",
       "      <td>18</td>\n",
       "      <td>580C5F90.png</td>\n",
       "      <td>848827C0.png</td>\n",
       "      <td>val</td>\n",
       "      <td>84586530.png</td>\n",
       "      <td>57EFD6E0.png</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>29371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>134</td>\n",
       "      <td>OS</td>\n",
       "      <td>19</td>\n",
       "      <td>5802E9B0.png</td>\n",
       "      <td>8480FBD0.png</td>\n",
       "      <td>val</td>\n",
       "      <td>84586530.png</td>\n",
       "      <td>57EFD6E0.png</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>29372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>134</td>\n",
       "      <td>OS</td>\n",
       "      <td>20</td>\n",
       "      <td>57FBBDC0.png</td>\n",
       "      <td>847514F0.png</td>\n",
       "      <td>val</td>\n",
       "      <td>84586530.png</td>\n",
       "      <td>57EFD6E0.png</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>29373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7010 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_patient side_eye  BScan   image_at_ti image_at_ti+1 split_type  \\\n",
       "0              3       OS      2  B3944840.png  AA8E1A00.png        val   \n",
       "1              3       OS      3  B38D4360.png  AA86EE10.png        val   \n",
       "2              3       OS      4  B3813570.png  AA7D7830.png        val   \n",
       "3              3       OS      5  B37A3090.png  AA73DB40.png        val   \n",
       "4              3       OS      6  B37304A0.png  AA6A6560.png        val   \n",
       "...          ...      ...    ...           ...           ...        ...   \n",
       "7005         134       OS     16  581D2870.png  849B3A90.png        val   \n",
       "7006         134       OS     17  58138B80.png  84919DA0.png        val   \n",
       "7007         134       OS     18  580C5F90.png  848827C0.png        val   \n",
       "7008         134       OS     19  5802E9B0.png  8480FBD0.png        val   \n",
       "7009         134       OS     20  57FBBDC0.png  847514F0.png        val   \n",
       "\n",
       "     LOCALIZER_at_ti+1 LOCALIZER_at_ti sex  age_at_ti+1  age_at_ti  \\\n",
       "0         A9D689D0.png    B2EB0FF0.png   M           81         80   \n",
       "1         A9D689D0.png    B2EB0FF0.png   M           81         80   \n",
       "2         A9D689D0.png    B2EB0FF0.png   M           81         80   \n",
       "3         A9D689D0.png    B2EB0FF0.png   M           81         80   \n",
       "4         A9D689D0.png    B2EB0FF0.png   M           81         80   \n",
       "...                ...             ...  ..          ...        ...   \n",
       "7005      84586530.png    57EFD6E0.png   M           66         66   \n",
       "7006      84586530.png    57EFD6E0.png   M           66         66   \n",
       "7007      84586530.png    57EFD6E0.png   M           66         66   \n",
       "7008      84586530.png    57EFD6E0.png   M           66         66   \n",
       "7009      84586530.png    57EFD6E0.png   M           66         66   \n",
       "\n",
       "      num_current_visit_at_i+1  num_current_visit_at_i  delta_t  label   case  \n",
       "0                            2                       1       28      1   1170  \n",
       "1                            2                       1       28      1   1171  \n",
       "2                            2                       1       28      1   1172  \n",
       "3                            2                       1       28      1   1173  \n",
       "4                            2                       1       28      1   1174  \n",
       "...                        ...                     ...      ...    ...    ...  \n",
       "7005                         3                       2       49      1  29369  \n",
       "7006                         3                       2       49      1  29370  \n",
       "7007                         3                       2       49      1  29371  \n",
       "7008                         3                       2       49      1  29372  \n",
       "7009                         3                       2       49      1  29373  \n",
       "\n",
       "[7010 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('csv/df_task1_val_challenge.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc86220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model_mae = MaskedAutoencoderViT()\n",
    "\n",
    "# chkpt_dir = \"/home/pzhang/OCT/code/mae/demo/mae_visualize_vit_large.pth\"\n",
    "# chkpt_dir =  \"/home/pzhang/OCT/code/mae/demo/mae_visualize_vit_large_ganloss.pth\"\n",
    "chkpt_dir = \"/home/pzhang/OCT/code/mae/weights/best_model_loss_v3.pth\"\n",
    "\n",
    "checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "\n",
    "msg = model_mae.load_state_dict(checkpoint, strict=False)\n",
    "print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b05a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image\n",
    "# img_url = \"/home/pzhang/Data/database/MARIO_CHALLENGE/data_1_update/val/C17F57D0.png\" #0\n",
    "# img_url = \"/home/pzhang/Data/database/MARIO_CHALLENGE/data_1_update/val/B35199F0.png\" #1\n",
    "# img_url = \"/home/pzhang/Data/database/MARIO_CHALLENGE/data_1_update/val/F47E68E0.png\" #2\n",
    "img_url = \"/home/pzhang/Data/database/MARIO_CHALLENGE/data_1_update/val/9DAC8170.png\" #3\n",
    "\n",
    "img_original = Image.open(img_url)  #img_original.size : (512, 496)\n",
    "img = img_original.resize((224, 224))\n",
    "img = np.array(img) / 255. #img.shape : (224, 224, 3) \n",
    "\n",
    "# normalize by ImageNet mean and std\n",
    "img = img - imagenet_mean\n",
    "img = img / imagenet_std\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [2, 2]\n",
    "show_image(torch.tensor(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict image\n",
    "def predict_one_image(img, model):\n",
    "    x = torch.tensor(img)\n",
    "    \n",
    "    # make it a batch-like\n",
    "    x = x.unsqueeze(dim=0)\n",
    "    x = torch.einsum('nhwc->nchw', x)\n",
    "\n",
    "    # run MAE EVO\n",
    "    loss, y, mask = model(x.float(), x.float(), mask_ratio=0.75)\n",
    "    \n",
    "    y = model.unpatchify(y)\n",
    "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
    "\n",
    "    return (y[0] * imagenet_std + imagenet_mean) * 255 #H W C # de-normalisation\n",
    "\n",
    "pred = predict_one_image(img, model_mae) #pred.numpy().shape : (224, 224, 3)\n",
    "\n",
    "\n",
    "# Convertir l'image en uint8\n",
    "pred = (pred.numpy()).astype(np.uint8)\n",
    "\n",
    "# Conversion de l'image en niveaux de gris\n",
    "pred = cv2.cvtColor(pred, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# resize into original shape \n",
    "pred = cv2.resize(pred, (img_original.size[0], img_original.size[1])) #pred.shape : (496, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6116c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resized image using OpenCV\n",
    "cv2.imwrite(\"pred_image.png\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172cf828",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t0 = cv2.imread(img_url , 0)\n",
    "img_t0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69434e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t1 = cv2.imread(\"pred_image.png\" , 0)\n",
    "img_t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d16b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce57618",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f70b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa4c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82868058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcbc899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 06:26:37.666131: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-08-30 06:26:37.666190: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2024-08-30 06:26:37.666223: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-08-30 06:26:37.666252: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-08-30 06:26:37.666278: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-08-30 06:26:37.675850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "# print(f\"Starting the inference for the team: {os.environ['Team_name']}\")\n",
    "\n",
    "label_type = \"multiclass\"\n",
    "image_size=(512,200) #W,H\n",
    "gray_scale = False\n",
    "\n",
    "# Load csv\n",
    "df = pd.read_csv('csv/task1_combined_data_mario_challenge_fold.csv')\n",
    "\n",
    "#test only\n",
    "df = df[:100]\n",
    "\n",
    "# Load data\n",
    "dataset = MARIO_DS_T2(df, mode=\"test\", image_size=(512,200) ,gray_scale=gray_scale, root_dir= \"/home/pzhang/Data/database/MARIO_CHALLENGE/data_1_update/train/\", processing_octip=False, mae_model=model_mae)\n",
    "\n",
    "# dataset = MARIO_DS('csv/task1_combined_data_mario_challenge_fold.csv', 'data/', transform=data_transforms['test'])\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Iterate over the first five batches in the DataLoader\n",
    "for batch_idx, (data1, data2, label, case) in enumerate(data_loader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Data T0:\", data1.shape)  # Displaying the shape of data1\n",
    "    print(\"Data T1:\", data2.shape)  # Displaying the shape of data2\n",
    "    print(\"Label:\", label.shape)    # Displaying the shape of labels\n",
    "    print(\"Case:\", case.shape)      # Displaying the shape of case\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Assuming data2 contains images and is in the form of a tensor\n",
    "    for i in range(min(5, data2.size(0))):  # Display up to 5 images from the batch\n",
    "        img = data2[i].cpu().numpy()  # Convert the tensor to a numpy array\n",
    "        img = img.transpose(1, 2, 0)  # Convert from (C, H, W) to (H, W, C) for plotting\n",
    "        \n",
    "        # Clip the values to be within the valid range for display\n",
    "        if img.dtype == 'float32' or img.dtype == 'float64':\n",
    "            img = np.clip(img, 0, 1)\n",
    "        else:  # Assuming integer data\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Batch {batch_idx + 1} - Image {i + 1}\")\n",
    "        plt.axis('off')  # Hide axis\n",
    "        plt.show()\n",
    "\n",
    "    if batch_idx == 1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213d852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f86a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.octip_segmentation_2d.octip.retina_localizer import RetinaLocalizer, RetinaLocalizationDataset\n",
    "from utils.octip_segmentation_2d.octip.pre_processor import PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.10.0\n",
    "# !pip install segmentation_models==1.0.1\n",
    "# !pip install opencv-python==4.4.0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70803e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load segementation models for preprocess\n",
    "model_directory = \"./utils/octip_models\"\n",
    "\n",
    "localizer1 = RetinaLocalizer('FPN','efficientnetb6',(384, 384),model_directory = model_directory)\n",
    "localizer2 = RetinaLocalizer('FPN', 'efficientnetb7', (320, 320),model_directory = model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e31eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f60152",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/home/pzhang/Data/database/MARIO_CHALLENGE/data_1_update/val/B3944840.png\"\n",
    "\n",
    "img_path2 = \"/home/pzhang/Data/database/MARIO_CHALLENGE/data_1_update/val/AA8E1A00.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmenting the retina of one B-scan with the first model \n",
    "_, seg1 = localizer1(RetinaLocalizationDataset([img_path,img_path2], 1, localizer1)) #shape (2,384,384,1) \n",
    "print(seg1.shape)\n",
    "seg1 = (seg1.squeeze()*255).astype(np.uint8)\n",
    "\n",
    "# segmenting the retina of one B-scan with the second model\n",
    "_, seg2 = localizer2(RetinaLocalizationDataset([img_path,img_path2], 1, localizer2)) #shape (2,320,320,1)\n",
    "seg2 = (seg2.squeeze()*255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = img_url\n",
    "img_path2 = \"pred_image.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9918f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmenting the retina of one B-scan with the first model \n",
    "_, seg1 = localizer1(RetinaLocalizationDataset([img_path,img_path2], 1, localizer1)) #shape (2,384,384,1) \n",
    "print(seg1.shape)\n",
    "seg1 = (seg1.squeeze()*255).astype(np.uint8)\n",
    "\n",
    "# segmenting the retina of one B-scan with the second model\n",
    "_, seg2 = localizer2(RetinaLocalizationDataset([img_path,img_path2], 1, localizer2)) #shape (2,320,320,1)\n",
    "seg2 = (seg2.squeeze()*255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(seg2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# # plt.imshow(seg1.squeeze(0))\n",
    "# plt.imshow((seg1.squeeze()*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759dbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(200, min_height = 100, normalize_intensities = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f33e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg1 = (seg1.squeeze()*255).astype(np.uint8)\n",
    "seg2 = (seg2.squeeze()*255).astype(np.uint8)\n",
    "\n",
    "img_t0 = preprocessor(img_path, [seg1[0], seg2[0]], output_directory='')\n",
    "img_t1 = preprocessor(img_path2, [seg1[1], seg2[1]], output_directory='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbaf851",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ed6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d619653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class InferenceTask1:\n",
    "    def __init__(self, model_paths, model_names, model_params, model_weights=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the inference class with model paths and weights.\n",
    "\n",
    "        Args:\n",
    "            model_paths (list): List of paths to the model files.\n",
    "            model_weights (list, optional): List of weights for each model. Defaults to equal weights.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models = [self.load_model(model_name,model_path,model_params) for model_name,model_path in zip(model_names,model_paths)]\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.i = 0\n",
    "        if model_weights is None:\n",
    "            self.model_weights = [1.0 / len(model_paths)] * len(model_paths)\n",
    "        else:\n",
    "            self.model_weights = model_weights\n",
    "\n",
    "    def load_model(self, model_name, model_path,model_params, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Loads a model from a given path and it's class name.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): name of the model class.\n",
    "            model_path (str): Path to the model file.\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "            torch.nn.Module: Loaded model.\n",
    "        \"\"\"\n",
    "        model = eval(model_name)(model_params['backbone'], model_params['pretrained'], model_params['num_classes'], model_params['model_type'] )\n",
    "        model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def simple_inference(self, data_loader):\n",
    "        \"\"\"\n",
    "        Performs inference on the data using the loaded model.\n",
    "\n",
    "        Args:\n",
    "            data_loader (DataLoader): DataLoader for the input data.\n",
    "\n",
    "        Returns:\n",
    "            list: True labels, predicted labels, and case IDs.\n",
    "        \"\"\"\n",
    "        \n",
    "        ## The proposed example only use the pair of OCT slice, but you are free to update if your pipeline involve\n",
    "        ## localizer and the clinical, udapte accordingly \n",
    "        \n",
    "        y_prob = []\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        cases = []\n",
    "        outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(data_loader)):\n",
    "                \n",
    "                imgs_t0, imgs_t1, labels, case_ids = data\n",
    "                imgs_t0 = imgs_t0.to(self.device).float()\n",
    "                imgs_t1 = imgs_t1.to(self.device).float()\n",
    "                \n",
    "                output = model(imgs_t0, imgs_t1)\n",
    "                \n",
    "                prediction = output.argmax(dim=1).item()\n",
    "                \n",
    "                y_pred.append(prediction)\n",
    "                y_true.append(labels)\n",
    "                \n",
    "                cases.append(case_ids)\n",
    "                \n",
    "        return y_true, y_pred, cases\n",
    "    \n",
    "    \n",
    "    \n",
    "    def scoring(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        DO NOT EDIT THIS CODE\n",
    "        Calculates various scoring metrics.\n",
    "\n",
    "        Args:\n",
    "            y_true (list): True labels.\n",
    "            y_pred (list): Predicted labels.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing various scores.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"F1_score\": f1_score(y_true, y_pred, average=\"micro\"),\n",
    "            \"Rk-correlation\": matthews_corrcoef(y_true, y_pred),\n",
    "            \"Specificity\": specificity(y_true, y_pred),\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def simple_ensemble_inference(self, data_loader):\n",
    "        \"\"\"\n",
    "        Performs inference using model ensembling and test time augmentation.\n",
    "\n",
    "        Args:\n",
    "            data_loader (DataLoader): DataLoader for the input data.\n",
    "\n",
    "        Returns:\n",
    "            list: True labels, predicted labels, and case IDs.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_prob = []\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        cases = []\n",
    "        outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(data_loader)):\n",
    "                \n",
    "                imgs_t0, imgs_t1, labels, case_ids = data\n",
    "                imgs_t0 = imgs_t0.to(self.device).float()\n",
    "                imgs_t1 = imgs_t1.to(self.device).float()\n",
    "                \n",
    "                print(labels)\n",
    "                \n",
    "                for model in self.models:\n",
    "                    \n",
    "                    output = model(imgs_t0, imgs_t1)\n",
    "                    outputs.append(output)\n",
    "                    \n",
    "                averaged_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "                prediction = list(averaged_output.argmax(dim=1).cpu().detach().numpy())\n",
    "                \n",
    "                y_pred.append(prediction)\n",
    "                y_true.append(labels.tolist())\n",
    "                \n",
    "                cases.append(case_ids.tolist())\n",
    "    \n",
    "        return y_true, y_pred, cases\n",
    "\n",
    "    def run(self, data_loader, use_ensemble = True):\n",
    "        \"\"\"\n",
    "        Runs the inference and saves results.\n",
    "\n",
    "        Args:\n",
    "            data_loader (DataLoader): DataLoader for the input data.\n",
    "            use_tta (bool): Whether to use test time augmentation.\n",
    "            n_augmentations (int): Number of augmentations to apply for TTA.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing various scores.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        ## You can test as much inference pipeline you which\n",
    "        # in your local machine. You will have to select\n",
    "        # two shot to for the final submission. \n",
    "        # The inference should always return a list of batch containing label,prediction,cases \n",
    "        # The method run should always return the scores\n",
    "        \n",
    "        if use_ensemble:\n",
    "            y_true, y_pred, cases = self.simple_ensemble_inference(data_loader)\n",
    "\n",
    "        else:\n",
    "            y_true, y_pred, cases = self.simple_inference(data_loader)\n",
    "            \n",
    "        # DO NOT EDIT THIS PART\n",
    "\n",
    "        y_true = functools.reduce(operator.iconcat, y_true, [])\n",
    "        y_pred = functools.reduce(operator.iconcat, y_pred, [])\n",
    "        cases = functools.reduce(operator.iconcat, cases, [])\n",
    "        \n",
    "#         output_file = f\"output/results_task1_team_{os.environ['Team_name']}_method_{self.i}.csv\"\n",
    "        output_file = f\"output/results_task1_team_df41_method_{self.i}.csv\"\n",
    "        df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'cases': cases})\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "        self.i +=1\n",
    "        return self.scoring(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8b146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_paths = ['models/task1_best_model_acc_f0.pth', 'models/task1_best_model_acc_f1.pth', 'models/task1_best_model_acc_f2.pth', 'models/task1_best_model_acc_f3.pth']  # Example for multiple models\n",
    "model_names = [\"MarioModelT1\", \"MarioModelT1\"]\n",
    "model_params = {\n",
    "    \"backbone\": \"resnet50\",\n",
    "    \"pretrained\": True,\n",
    "    \"num_classes\": 4,\n",
    "    \"model_type\": \"features_fusion\"\n",
    "}\n",
    "\n",
    "# model_weights_contribution = [0.6, 0.4]  # Example weights for the models\n",
    "inference_task1 = InferenceTask1(model_paths, model_names, model_params)\n",
    "\n",
    "scores_1 = inference_task1.run(data_loader, use_ensemble = True)\n",
    "print(f\"Obtained scores for inference method 1: F1_score: {scores_1['F1_score']}, Rk-correlation: {scores_1['Rk-correlation']}, Specificity: {scores_1['Specificity']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15efc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results saved to output/results_task1_team_df41_method_0.csv\n",
    "# Obtained scores for inference method 1: F1_score: 0.7699999999999999, Rk-correlation: 0.0, Specificity: 0.6666666590928542\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ceb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output/results_task1_team_df41_method_0.csv')\n",
    "df.y_pred.tolist() == df.prediction.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9152508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the columns 'y_pred' and 'prediction' to find rows where they are not equal\n",
    "mismatch_rows = df[df['y_pred'] != df['prediction']]\n",
    "\n",
    "# Display the rows that are not identical\n",
    "print(mismatch_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab155d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
