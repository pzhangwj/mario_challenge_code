{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc796e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 11:01:49.922907: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-10 11:01:50.188359: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-10 11:01:50.276313: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-10 11:01:51.064082: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-10 11:01:51.064201: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-10 11:01:51.064207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MARIO_DS' from 'utils.dataset' (/home/pzhang/Challenges/MARIO/MARIO-Challenge-MICCAI-2024/git/mario_challenge_code/utils/dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score, cohen_kappa_score, matthews_corrcoef\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MARIO_DS\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscoring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m specificity, compute_metrics, plot_confusion_matrix\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MARIO_DS' from 'utils.dataset' (/home/pzhang/Challenges/MARIO/MARIO-Challenge-MICCAI-2024/git/mario_challenge_code/utils/dataset.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score, matthews_corrcoef\n",
    "from utils.dataset import MARIO_DS\n",
    "from utils.scoring import specificity, compute_metrics, plot_confusion_matrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "from models.model import MARIO_DS_T1\n",
    "# from models.example_model_task1v2 import SimpleModel1v2\n",
    "\n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision.transforms as T\n",
    "\n",
    "import operator\n",
    "import functools\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/df_task1_val_challenge.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602fefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcbc899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "# print(f\"Starting the inference for the team: {os.environ['Team_name']}\")\n",
    "\n",
    "label_type = \"multiclass\"\n",
    "image_size=(512,200) #W,H\n",
    "gray_scale = False\n",
    "\n",
    "# Load csv\n",
    "df = pd.read_csv('csv/task1_combined_data_mario_challenge_fold.csv')\n",
    "\n",
    "#test only\n",
    "df = df[:100]\n",
    "\n",
    "# Load data\n",
    "dataset = MARIO_DS(df, mode=\"test\", image_size=(512,200) ,gray_scale=gray_scale, root_dir= \"/home/pzhang/Data/database/MARIO_CHALLENGE/data_1_update/train/\", processing_octip=True)\n",
    "\n",
    "# dataset = MARIO_DS('csv/task1_combined_data_mario_challenge_fold.csv', 'data/', transform=data_transforms['test'])\n",
    "data_loader = DataLoader(dataset, batch_size=20, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Iterate over the first five batches in the DataLoader\n",
    "for batch_idx, (data1, data2, label, case) in enumerate(data_loader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Data T0:\", data1.shape)  # Displaying the shape of data1\n",
    "    print(\"Data T1:\", data2.shape)  # Displaying the shape of data2\n",
    "    print(\"Label:\", label.shape)    # Displaying the shape of labels\n",
    "    print(\"Case:\", case.shape)      # Displaying the shape of case\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Assuming data2 contains images and is in the form of a tensor\n",
    "    for i in range(min(5, data2.size(0))):  # Display up to 5 images from the batch\n",
    "        img = data2[i].cpu().numpy()  # Convert the tensor to a numpy array\n",
    "        img = img.transpose(1, 2, 0)  # Convert from (C, H, W) to (H, W, C) for plotting\n",
    "        \n",
    "        # Clip the values to be within the valid range for display\n",
    "        if img.dtype == 'float32' or img.dtype == 'float64':\n",
    "            img = np.clip(img, 0, 1)\n",
    "        else:  # Assuming integer data\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Batch {batch_idx + 1} - Image {i + 1}\")\n",
    "        plt.axis('off')  # Hide axis\n",
    "        plt.show()\n",
    "\n",
    "    if batch_idx == 1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213d852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f86a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.octip_segmentation_2d.octip.retina_localizer import RetinaLocalizer, RetinaLocalizationDataset\n",
    "from utils.octip_segmentation_2d.octip.pre_processor import PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.10.0\n",
    "# !pip install segmentation_models==1.0.1\n",
    "# !pip install opencv-python==4.4.0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70803e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load segementation models for preprocess\n",
    "model_directory = \"./utils/octip_models\"\n",
    "\n",
    "localizer1 = RetinaLocalizer('FPN','efficientnetb6',(384, 384),model_directory = model_directory)\n",
    "localizer2 = RetinaLocalizer('FPN', 'efficientnetb7', (320, 320),model_directory = model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e31eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f60152",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/home/pzhang/Data/database/MARIO_CHALLENGE/data_1_update/val/B3944840.png\"\n",
    "\n",
    "img_path2 = \"/home/pzhang/Data/database/MARIO_CHALLENGE/data_1_update/val/AA8E1A00.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmenting the retina of one B-scan with the first model \n",
    "_, seg1 = localizer1(RetinaLocalizationDataset([img_path,img_path2], 1, localizer1)) #shape (2,384,384,1) \n",
    "print(seg1.shape)\n",
    "seg1 = (seg1.squeeze()*255).astype(np.uint8)\n",
    "\n",
    "# segmenting the retina of one B-scan with the second model\n",
    "_, seg2 = localizer2(RetinaLocalizationDataset([img_path,img_path2], 1, localizer2)) #shape (2,320,320,1)\n",
    "seg2 = (seg2.squeeze()*255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(seg1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# # plt.imshow(seg1.squeeze(0))\n",
    "# plt.imshow((seg1.squeeze()*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759dbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(200, min_height = 100, normalize_intensities = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f33e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg1 = (seg1.squeeze()*255).astype(np.uint8)\n",
    "seg2 = (seg2.squeeze()*255).astype(np.uint8)\n",
    "\n",
    "img_t0 = preprocessor(img_path, [seg1[0], seg2[0]], output_directory='')\n",
    "img_t1 = preprocessor(img_path2, [seg1[1], seg2[1]], output_directory='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbaf851",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39952aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ed6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d619653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class InferenceTask1:\n",
    "    def __init__(self, model_paths, model_names, model_params, model_weights=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the inference class with model paths and weights.\n",
    "\n",
    "        Args:\n",
    "            model_paths (list): List of paths to the model files.\n",
    "            model_weights (list, optional): List of weights for each model. Defaults to equal weights.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.models = [self.load_model(model_name,model_path,model_params) for model_name,model_path in zip(model_names,model_paths)]\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.i = 0\n",
    "        if model_weights is None:\n",
    "            self.model_weights = [1.0 / len(model_paths)] * len(model_paths)\n",
    "        else:\n",
    "            self.model_weights = model_weights\n",
    "\n",
    "    def load_model(self, model_name, model_path,model_params, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Loads a model from a given path and it's class name.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): name of the model class.\n",
    "            model_path (str): Path to the model file.\n",
    "            \n",
    "\n",
    "        Returns:\n",
    "            torch.nn.Module: Loaded model.\n",
    "        \"\"\"\n",
    "        model = eval(model_name)(model_params['backbone'], model_params['pretrained'], model_params['num_classes'], model_params['model_type'] )\n",
    "        model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def simple_inference(self, data_loader):\n",
    "        \"\"\"\n",
    "        Performs inference on the data using the loaded model.\n",
    "\n",
    "        Args:\n",
    "            data_loader (DataLoader): DataLoader for the input data.\n",
    "\n",
    "        Returns:\n",
    "            list: True labels, predicted labels, and case IDs.\n",
    "        \"\"\"\n",
    "        \n",
    "        ## The proposed example only use the pair of OCT slice, but you are free to update if your pipeline involve\n",
    "        ## localizer and the clinical, udapte accordingly \n",
    "        \n",
    "        y_prob = []\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        cases = []\n",
    "        outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(data_loader)):\n",
    "                \n",
    "                imgs_t0, imgs_t1, labels, case_ids = data\n",
    "                imgs_t0 = imgs_t0.to(self.device).float()\n",
    "                imgs_t1 = imgs_t1.to(self.device).float()\n",
    "                \n",
    "                output = model(imgs_t0, imgs_t1)\n",
    "                \n",
    "                prediction = output.argmax(dim=1).item()\n",
    "                \n",
    "                y_pred.append(prediction)\n",
    "                y_true.append(labels)\n",
    "                \n",
    "                cases.append(case_ids)\n",
    "                \n",
    "        return y_true, y_pred, cases\n",
    "    \n",
    "    \n",
    "    \n",
    "    def scoring(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        DO NOT EDIT THIS CODE\n",
    "        Calculates various scoring metrics.\n",
    "\n",
    "        Args:\n",
    "            y_true (list): True labels.\n",
    "            y_pred (list): Predicted labels.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing various scores.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"F1_score\": f1_score(y_true, y_pred, average=\"micro\"),\n",
    "            \"Rk-correlation\": matthews_corrcoef(y_true, y_pred),\n",
    "            \"Specificity\": specificity(y_true, y_pred),\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def simple_ensemble_inference(self, data_loader):\n",
    "        \"\"\"\n",
    "        Performs inference using model ensembling and test time augmentation.\n",
    "\n",
    "        Args:\n",
    "            data_loader (DataLoader): DataLoader for the input data.\n",
    "\n",
    "        Returns:\n",
    "            list: True labels, predicted labels, and case IDs.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_prob = []\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        cases = []\n",
    "        outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(data_loader)):\n",
    "                \n",
    "                imgs_t0, imgs_t1, labels, case_ids = data\n",
    "                imgs_t0 = imgs_t0.to(self.device).float()\n",
    "                imgs_t1 = imgs_t1.to(self.device).float()\n",
    "                \n",
    "                print(labels)\n",
    "                \n",
    "                for model in self.models:\n",
    "                    \n",
    "                    output = model(imgs_t0, imgs_t1)\n",
    "                    outputs.append(output)\n",
    "                    \n",
    "                averaged_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "                prediction = list(averaged_output.argmax(dim=1).cpu().detach().numpy())\n",
    "                \n",
    "                y_pred.append(prediction)\n",
    "                y_true.append(labels.tolist())\n",
    "                \n",
    "                cases.append(case_ids.tolist())\n",
    "    \n",
    "        return y_true, y_pred, cases\n",
    "\n",
    "    def run(self, data_loader, use_ensemble = True):\n",
    "        \"\"\"\n",
    "        Runs the inference and saves results.\n",
    "\n",
    "        Args:\n",
    "            data_loader (DataLoader): DataLoader for the input data.\n",
    "            use_tta (bool): Whether to use test time augmentation.\n",
    "            n_augmentations (int): Number of augmentations to apply for TTA.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing various scores.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        ## You can test as much inference pipeline you which\n",
    "        # in your local machine. You will have to select\n",
    "        # two shot to for the final submission. \n",
    "        # The inference should always return a list of batch containing label,prediction,cases \n",
    "        # The method run should always return the scores\n",
    "        \n",
    "        if use_ensemble:\n",
    "            y_true, y_pred, cases = self.simple_ensemble_inference(data_loader)\n",
    "\n",
    "        else:\n",
    "            y_true, y_pred, cases = self.simple_inference(data_loader)\n",
    "            \n",
    "        # DO NOT EDIT THIS PART\n",
    "\n",
    "        y_true = functools.reduce(operator.iconcat, y_true, [])\n",
    "        y_pred = functools.reduce(operator.iconcat, y_pred, [])\n",
    "        cases = functools.reduce(operator.iconcat, cases, [])\n",
    "        \n",
    "#         output_file = f\"output/results_task1_team_{os.environ['Team_name']}_method_{self.i}.csv\"\n",
    "        output_file = f\"output/results_task1_team_df41_method_{self.i}.csv\"\n",
    "        df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'cases': cases})\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "        self.i +=1\n",
    "        return self.scoring(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8b146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_paths = ['models/task1_best_model_acc_f0.pth', 'models/task1_best_model_acc_f1.pth', 'models/task1_best_model_acc_f2.pth', 'models/task1_best_model_acc_f3.pth']  # Example for multiple models\n",
    "model_names = [\"MarioModelT1\", \"MarioModelT1\"]\n",
    "model_params = {\n",
    "    \"backbone\": \"resnet50\",\n",
    "    \"pretrained\": True,\n",
    "    \"num_classes\": 4,\n",
    "    \"model_type\": \"features_fusion\"\n",
    "}\n",
    "\n",
    "# model_weights_contribution = [0.6, 0.4]  # Example weights for the models\n",
    "inference_task1 = InferenceTask1(model_paths, model_names, model_params)\n",
    "\n",
    "scores_1 = inference_task1.run(data_loader, use_ensemble = True)\n",
    "print(f\"Obtained scores for inference method 1: F1_score: {scores_1['F1_score']}, Rk-correlation: {scores_1['Rk-correlation']}, Specificity: {scores_1['Specificity']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15efc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results saved to output/results_task1_team_df41_method_0.csv\n",
    "# Obtained scores for inference method 1: F1_score: 0.7699999999999999, Rk-correlation: 0.0, Specificity: 0.6666666590928542\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ceb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output/results_task1_team_df41_method_0.csv')\n",
    "df.y_pred.tolist() == df.prediction.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9152508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the columns 'y_pred' and 'prediction' to find rows where they are not equal\n",
    "mismatch_rows = df[df['y_pred'] != df['prediction']]\n",
    "\n",
    "# Display the rows that are not identical\n",
    "print(mismatch_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab155d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
